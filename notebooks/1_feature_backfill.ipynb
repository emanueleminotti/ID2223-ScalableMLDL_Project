{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1f2881-7273-45ff-9259-537b23ad8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Root dir: /Users/emaminotti/ID2223-ScalableMLDL_Project\n",
      "Added the following directory to the PYTHONPATH: /Users/emaminotti/ID2223-ScalableMLDL_Project\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "root_dir = Path().absolute()\n",
    "# Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "if root_dir.parts[-1:] == ('notebooks',):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "root_dir = str(root_dir) \n",
    "print(\"Local environment\")\n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` \n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "\n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6a80c",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f447120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "import util\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b8d1a-62a5-4a1d-b805-6e83cafcd29f",
   "metadata": {},
   "source": [
    "## Hopsworks API Key\n",
    "You need to have registered an account on app.hopsworks.ai.\n",
    "\n",
    "Save the HOPSWORKS_API_KEY  to ~/.env file in the root directory of your project\n",
    "\n",
    "In the .env file, update HOPSWORKS_API_KEY:\n",
    "\n",
    "`HOPSWORKS_API_KEY=\"put API KEY value in this string\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1a49d6-9cd2-4246-b0ca-1058672e4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-20 16:07:22,014 INFO: Initializing external client\n",
      "2025-12-20 16:07:22,015 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-20 16:07:23,474 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1279152\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9145f0b7-d961-41f7-aebe-741dbf00784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hopsworks import RestAPIError\n",
    "\n",
    "api_url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "country = \"Sweden\"\n",
    "city = \"Stockholm\"\n",
    "latitude = 59.3346 \n",
    "longitude = 18.0632"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706e751",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Read the historical data into a DataFrame </span>\n",
    "\n",
    "The cell below will read up historical pollen levels data into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3a1212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pollen data for Stockholm from 2013-01-01 to 2025-12-20...\n",
      "Data fetched successfully!\n",
      "                 time  alder_pollen  birch_pollen  grass_pollen  \\\n",
      "0 2013-01-01 00:00:00           NaN           NaN           NaN   \n",
      "1 2013-01-01 01:00:00           NaN           NaN           NaN   \n",
      "2 2013-01-01 02:00:00           NaN           NaN           NaN   \n",
      "3 2013-01-01 03:00:00           NaN           NaN           NaN   \n",
      "4 2013-01-01 04:00:00           NaN           NaN           NaN   \n",
      "\n",
      "   mugwort_pollen  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "Total rows: 113688\n"
     ]
    }
   ],
   "source": [
    "# Define the date range for historical data\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the pollen variables available in Open-Meteo\n",
    "# We don't consider olive and ragweed because they are negligible\n",
    "pollen_vars = [\n",
    "    \"alder_pollen\", \n",
    "    \"birch_pollen\", \n",
    "    \"grass_pollen\", \n",
    "    \"mugwort_pollen\"\n",
    "]\n",
    "\n",
    "# Set up the API parameters\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"hourly\": \",\".join(pollen_vars), # Request all pollen types\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"timezone\": \"auto\" # Automatically match the city's timezone\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "print(f\"Fetching pollen data for {city} from {start_date} to {end_date}...\")\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    response_json = response.json()\n",
    "    \n",
    "    # Convert the 'hourly' data into a Pandas DataFrame\n",
    "    hourly_data = response_json[\"hourly\"]\n",
    "    df_pollen = pd.DataFrame(hourly_data)\n",
    "    \n",
    "    # Convert the 'time' column to datetime objects and set it as the index\n",
    "    df_pollen[\"time\"] = pd.to_datetime(df_pollen[\"time\"])\n",
    "    \n",
    "    print(\"Data fetched successfully!\")\n",
    "    print(df_pollen.head())\n",
    "    print(f\"Total rows: {len(df_pollen)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812eb37-04e3-4291-8d77-a69ef7a195bc",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Data cleaning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd20c859-ef3c-4b54-bbcb-83898afefa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and interpolation complete.\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns\n",
    "cols_to_keep = ['time', 'alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']\n",
    "df_pollen_clean = df_pollen[cols_to_keep].copy()\n",
    "\n",
    "# Set index and Resample to Daily Mean\n",
    "df_pollen_clean.set_index('time', inplace=True)\n",
    "df_pollen_daily = df_pollen_clean.resample('D').mean()\n",
    "\n",
    "# Linear Interpolation for gaps\n",
    "# limit_direction='both' ensures we fill gaps based on surrounding data\n",
    "df_pollen_daily = df_pollen_daily.interpolate(method='time', limit_direction='both')\n",
    "\n",
    "df_pollen_daily.dropna(inplace=True)\n",
    "\n",
    "# Cast types\n",
    "pollen_cols = ['alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']\n",
    "df_pollen_daily[pollen_cols] = df_pollen_daily[pollen_cols].astype('float32')\n",
    "\n",
    "# Reset index\n",
    "df_pollen_daily.reset_index(inplace=True)\n",
    "df_pollen_daily.rename(columns={'time': 'date'}, inplace=True)\n",
    "\n",
    "# Add country and city columns\n",
    "df_pollen_daily['country'] = country\n",
    "df_pollen_daily['city'] = city\n",
    "\n",
    "print(\"Data cleaning and interpolation complete.\")\n",
    "print(f\"Remaining missing values: {df_pollen_daily.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa32b9",
   "metadata": {},
   "source": [
    "### Adding lagged features\n",
    "\n",
    "To capture short-term temporal dependencies, we add three new features representing the pollen levels of the previous 1, 2, and 3 days.\n",
    "These lagged values will help the model learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba7dfa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with lagged features:\n",
      "        date  birch_pollen  birch_pollen_lag_1  birch_pollen_lag_2\n",
      "0 2013-01-04           0.0                 0.0                 0.0\n",
      "1 2013-01-05           0.0                 0.0                 0.0\n",
      "2 2013-01-06           0.0                 0.0                 0.0\n",
      "3 2013-01-07           0.0                 0.0                 0.0\n",
      "4 2013-01-08           0.0                 0.0                 0.0\n",
      "\n",
      "New DataFrame shape: (4734, 19)\n"
     ]
    }
   ],
   "source": [
    "# Define the pollen columns we want to lag\n",
    "pollen_cols = ['alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']\n",
    "\n",
    "# Create the lagged features\n",
    "for col in pollen_cols:\n",
    "    for lag in range(1, 4):  # Loops through 1, 2, 3\n",
    "        # Name the new column, e.g., 'alder_pollen_lag_1'\n",
    "        lag_col_name = f\"{col}_lag_{lag}\"\n",
    "        # Shift the column data down by 'lag' rows\n",
    "        df_pollen_daily[lag_col_name] = df_pollen_daily[col].shift(lag)\n",
    "\n",
    "# Drop the first 3 rows which now contain NaNs (since we shifted up to 3 days)\n",
    "df_pollen_daily.dropna(inplace=True)\n",
    "\n",
    "# Reset index is generally good practice after dropping rows, though not strictly necessary if date is a column\n",
    "df_pollen_daily.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check the result\n",
    "print(\"Data with lagged features:\")\n",
    "print(df_pollen_daily[['date', 'birch_pollen', 'birch_pollen_lag_1', 'birch_pollen_lag_2']].head())\n",
    "print(f\"\\nNew DataFrame shape: {df_pollen_daily.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e8276",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055befa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style='color:#ff5f27'> Loading Weather Data from [Open Meteo](https://open-meteo.com/en/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78686a28",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Download the Historical Weather Data </span>\n",
    "\n",
    "\n",
    "We will download the historical weather data for your `city` by first extracting the earliest date from your DataFrame containing the historical air quality measurements.\n",
    "\n",
    "We will download all daily historical weather data measurements for your `city` from the earliest date in your air quality measurement DataFrame. It doesn't matter if there are missing days of air quality measurements. We can store all of the daily weather measurements, and when we build our training dataset, we will join up the air quality measurements for a given day to its weather features for that day. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d604b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical weather data from 2013-01-04 to 2025-12-20...\n",
      "Weather data fetched successfully!\n",
      "        date  temperature_2m_max  temperature_2m_min  temperature_2m_mean  \\\n",
      "0 2013-01-04                 2.7                -0.3                  1.6   \n",
      "1 2013-01-05                -0.4                -3.6                 -2.2   \n",
      "2 2013-01-06                -0.2                -3.8                 -1.7   \n",
      "3 2013-01-07                 0.8                -4.8                 -1.2   \n",
      "4 2013-01-08                 1.3                -2.5                  0.0   \n",
      "\n",
      "   precipitation_sum  rain_sum  snowfall_sum  wind_speed_10m_max  \\\n",
      "0                0.0       0.0          0.00           23.200001   \n",
      "1                0.0       0.0          0.00           16.900000   \n",
      "2                0.0       0.0          0.00            9.400000   \n",
      "3                0.0       0.0          0.00           15.100000   \n",
      "4                2.5       0.1          1.75           18.400000   \n",
      "\n",
      "   wind_direction_10m_dominant  weather_code country       city  \n",
      "0                        309.0           3.0  Sweden  Stockholm  \n",
      "1                        322.0           3.0  Sweden  Stockholm  \n",
      "2                         30.0           3.0  Sweden  Stockholm  \n",
      "3                        219.0           3.0  Sweden  Stockholm  \n",
      "4                        247.0          73.0  Sweden  Stockholm  \n",
      "Shape: (4734, 12)\n"
     ]
    }
   ],
   "source": [
    "# Get the earliest date from the pollen DataFrame\n",
    "start_date = df_pollen_daily['date'].min().strftime('%Y-%m-%d')\n",
    "end_date = datetime.date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Fetching historical weather data from {start_date} to {end_date}...\")\n",
    "\n",
    "# Define the Weather API URL (Archive API for history)\n",
    "weather_api_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define relevant weather variables for pollen prediction\n",
    "weather_vars = [\n",
    "    \"temperature_2m_max\",\n",
    "    \"temperature_2m_min\", \n",
    "    \"temperature_2m_mean\",\n",
    "    \"precipitation_sum\",\n",
    "    \"rain_sum\",\n",
    "    \"snowfall_sum\",\n",
    "    \"wind_speed_10m_max\",\n",
    "    \"wind_direction_10m_dominant\",\n",
    "    \"weather_code\"\n",
    "]\n",
    "\n",
    "# Set up the API parameters\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"daily\": \",\".join(weather_vars),\n",
    "    \"timezone\": \"auto\"\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(weather_api_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_json = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    daily_data = response_json[\"daily\"]\n",
    "    df_weather = pd.DataFrame(daily_data)\n",
    "    \n",
    "    # Data Cleaning for Weather Data\n",
    "    # Convert 'time' to datetime\n",
    "    df_weather[\"time\"] = pd.to_datetime(df_weather[\"time\"])\n",
    "    df_weather.rename(columns={\"time\": \"date\"}, inplace=True)\n",
    "    \n",
    "    # Cast float columns to float32 to save memory\n",
    "    float_cols = [c for c in df_weather.columns if c != \"date\"]\n",
    "    df_weather[float_cols] = df_weather[float_cols].astype(\"float32\")\n",
    "\n",
    "    df_weather[\"country\"] = country\n",
    "    df_weather[\"city\"] = city\n",
    "    \n",
    "    print(\"Weather data fetched successfully!\")\n",
    "    print(df_weather.head())\n",
    "    print(f\"Shape: {df_weather.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to fetch weather data. Status: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd6eefe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4734 entries, 0 to 4733\n",
      "Data columns (total 12 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         4734 non-null   datetime64[ns]\n",
      " 1   temperature_2m_max           4734 non-null   float32       \n",
      " 2   temperature_2m_min           4734 non-null   float32       \n",
      " 3   temperature_2m_mean          4734 non-null   float32       \n",
      " 4   precipitation_sum            4734 non-null   float32       \n",
      " 5   rain_sum                     4734 non-null   float32       \n",
      " 6   snowfall_sum                 4734 non-null   float32       \n",
      " 7   wind_speed_10m_max           4734 non-null   float32       \n",
      " 8   wind_direction_10m_dominant  4734 non-null   float32       \n",
      " 9   weather_code                 4734 non-null   float32       \n",
      " 10  country                      4734 non-null   object        \n",
      " 11  city                         4734 non-null   object        \n",
      "dtypes: datetime64[ns](1), float32(9), object(2)\n",
      "memory usage: 277.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f77aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e79b3f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> Create the Feature Groups and insert the DataFrames in them </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d2bb403",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "FeatureStoreException",
     "evalue": "Failed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestAPIError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hopsworks_common/core/variable_api.py:126\u001b[39m, in \u001b[36mVariableApi.get_loadbalancer_external_domain\u001b[39m\u001b[34m(self, service)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloadbalancer_external_domain_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mservice\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hopsworks_common/core/variable_api.py:52\u001b[39m, in \u001b[36mVariableApi.get_variable\u001b[39m\u001b[34m(self, variable)\u001b[39m\n\u001b[32m     51\u001b[39m path_params = [\u001b[33m\"\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m\"\u001b[39m, variable]\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m domain = \u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m domain[\u001b[33m\"\u001b[39m\u001b[33msuccessMessage\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hopsworks_common/decorators.py:48\u001b[39m, in \u001b[36mconnected.<locals>.if_connected\u001b[39m\u001b[34m(inst, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hopsworks_common/client/base.py:186\u001b[39m, in \u001b[36mClient._send_request\u001b[39m\u001b[34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code // \u001b[32m100\u001b[39m != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.RestAPIError(url, response)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[31mRestAPIError\u001b[39m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/variables/loadbalancer_external_domain_datanode). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":100050,\"usrMsg\":\"Variable: loadbalancer_external_domain_datanodenot found\",\"errorMsg\":\"Requested variable not found\"}', error code: 100050, error msg: Requested variable not found, user msg: Variable: loadbalancer_external_domain_datanodenot found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFeatureStoreException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/delta_engine.py:285\u001b[39m, in \u001b[36mDeltaEngine._setup_delta_rs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     datanode_ip = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loadbalancer_external_domain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdatanode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     _logger.debug(\n\u001b[32m    289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSetting HOPSFS_CLOUD_DATANODE_HOSTNAME_OVERRIDE to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatanode_ip\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hopsworks_common/core/variable_api.py:129\u001b[39m, in \u001b[36mVariableApi.get_loadbalancer_external_domain\u001b[39m\u001b[34m(self, service)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err.STATUS_CODE_NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClient could not get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLOADBALANCER_SERVICES[service]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service hostname from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloadbalancer_external_domain_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe variable is either not set or empty in Hopsworks cluster configuration.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFeatureStoreException\u001b[39m: Client could not get datanode service hostname from loadbalancer_external_domain_datanode. The variable is either not set or empty in Hopsworks cluster configuration.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFeatureStoreException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m pollen_fg = fs.get_or_create_feature_group(\n\u001b[32m      3\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33mpollen_measurements\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     description=\u001b[33m'\u001b[39m\u001b[33mDaily average pollen levels for Stockholm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     event_time=\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Insert the DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mpollen_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pollen_daily\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Add Descriptions for Documentation\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# It is best practice to document units (grains/m¬≥) so others understand the values.\u001b[39;00m\n\u001b[32m     15\u001b[39m pollen_fg.update_feature_description(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDate of the pollen measurement (Timestamp)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/feature_group.py:3454\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m   3445\u001b[39m     [\n\u001b[32m   3446\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3450\u001b[39m ):\n\u001b[32m   3451\u001b[39m     \u001b[38;5;66;03m# New delta FG allow for change data capture query\u001b[39;00m\n\u001b[32m   3452\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33mdelta.enableChangeDataFeed\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3454\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3455\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3457\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[38;5;66;03m# Compute stats in client if there is no backfill job:\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;66;03m# - spark engine: always compute in client\u001b[39;00m\n\u001b[32m   3468\u001b[39m \u001b[38;5;66;03m# - python engine: only compute if FG is offline only (no backfill job)\u001b[39;00m\n\u001b[32m   3469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/feature_group_engine.py:247\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28mself\u001b[39m._feature_group_api.delete_content(feature_group)\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbulk_insert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    256\u001b[39m     ge_report,\n\u001b[32m    257\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/engine/python.py:1084\u001b[39m, in \u001b[36mEngine.save_dataframe\u001b[39m\u001b[34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine.get_type() == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_group.time_travel_format == \u001b[33m\"\u001b[39m\u001b[33mDELTA\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m         delta_engine_instance = \u001b[43mdelta_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDeltaEngine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_store_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_store_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspark_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspark_session\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m         delta_engine_instance.save_delta_fg(\n\u001b[32m   1092\u001b[39m             dataframe,\n\u001b[32m   1093\u001b[39m             write_options=offline_write_options,\n\u001b[32m   1094\u001b[39m             validation_id=validation_id,\n\u001b[32m   1095\u001b[39m         )\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/delta_engine.py:76\u001b[39m, in \u001b[36mDeltaEngine.__init__\u001b[39m\u001b[34m(self, feature_store_id, feature_store_name, feature_group, spark_session, spark_context)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m._variable_api = variable_api.VariableApi()\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m._project_api = project_api.ProjectApi()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_delta_rs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/delta_engine.py:293\u001b[39m, in \u001b[36mDeltaEngine._setup_delta_rs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mHOPSFS_CLOUD_DATANODE_HOSTNAME_OVERRIDE\u001b[39m\u001b[33m\"\u001b[39m] = datanode_ip\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FeatureStoreException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[32m    294\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    297\u001b[39m user_name = \u001b[38;5;28mself\u001b[39m._project_api.get_user_info().get(\u001b[33m\"\u001b[39m\u001b[33musername\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m user_name:\n",
      "\u001b[31mFeatureStoreException\u001b[39m: Failed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster."
     ]
    }
   ],
   "source": [
    "# Get or Create the Feature Group\n",
    "pollen_fg = fs.get_or_create_feature_group(\n",
    "    name='pollen_measurements',\n",
    "    description='Daily average pollen levels for Stockholm',\n",
    "    version=1,\n",
    "    primary_key=['country', 'city'],\n",
    "    event_time=\"date\" \n",
    ")\n",
    "\n",
    "# Insert the DataFrame\n",
    "pollen_fg.insert(df_pollen_daily)\n",
    "\n",
    "# Add Descriptions for Documentation\n",
    "# It is best practice to document units (grains/m¬≥) so others understand the values.\n",
    "pollen_fg.update_feature_description(\"date\", \"Date of the pollen measurement (Timestamp)\")\n",
    "pollen_fg.update_feature_description(\"alder_pollen\", \"Daily average Alder pollen concentration (grains/m¬≥)\")\n",
    "pollen_fg.update_feature_description(\"birch_pollen\", \"Daily average Birch pollen concentration (grains/m¬≥)\")\n",
    "pollen_fg.update_feature_description(\"grass_pollen\", \"Daily average Grass pollen concentration (grains/m¬≥)\")\n",
    "pollen_fg.update_feature_description(\"mugwort_pollen\", \"Daily average Mugwort pollen concentration (grains/m¬≥)\")\n",
    "\n",
    "# Add descriptions for the lagged features\n",
    "for pollen_type in ['alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']:\n",
    "    for i in range(1, 4):\n",
    "        col_name = f\"{pollen_type}_lag_{i}\"\n",
    "        if col_name in df_pollen_daily.columns:\n",
    "            pollen_fg.update_feature_description(col_name, f\"{pollen_type} concentration {i} day(s) prior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894b731",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a84ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation Suite created and attached.\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1267872/featurestores/1254483/featuregroups). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"errorCode\":270066,\"usrMsg\":\"project: ScalableMLDL, database: scalablemldl, db user:ScalableMLDL_emamino0, jdbcString: jdbc:mysql://10.2.8.180:3306/scalablemldl?useSSL=false&allowPublicKeyRetrieval=true\",\"devMsg\":\"Access denied for user \\'ScalableMLDL_emamino0\\'@\\'10-2-5-109.hopsworks-internal.hopsworks.svc.cluster.local\\' (using password: YES)\",\"errorMsg\":\"Could not initiate connection to MySQL Server\"}', error code: 270066, error msg: Could not initiate connection to MySQL Server, user msg: project: ScalableMLDL, database: scalablemldl, db user:ScalableMLDL_emamino0, jdbcString: jdbc:mysql://10.2.8.180:3306/scalablemldl?useSSL=false&allowPublicKeyRetrieval=true",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestAPIError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExpectation Suite created and attached.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Insert Data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[43mweather_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_weather\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Update Feature Descriptions\u001b[39;00m\n\u001b[32m     64\u001b[39m weather_fg.update_feature_description(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDate of the weather measurement (Timestamp)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/feature_group.py:3454\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m   3445\u001b[39m     [\n\u001b[32m   3446\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3450\u001b[39m ):\n\u001b[32m   3451\u001b[39m     \u001b[38;5;66;03m# New delta FG allow for change data capture query\u001b[39;00m\n\u001b[32m   3452\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33mdelta.enableChangeDataFeed\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3454\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3455\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3457\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[38;5;66;03m# Compute stats in client if there is no backfill job:\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;66;03m# - spark engine: always compute in client\u001b[39;00m\n\u001b[32m   3468\u001b[39m \u001b[38;5;66;03m# - python engine: only compute if FG is offline only (no backfill job)\u001b[39;00m\n\u001b[32m   3469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/feature_group_engine.py:206\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    200\u001b[39m     dataframe_features = DataFrameValidator().validate_schema(\n\u001b[32m    201\u001b[39m         feature_group, feature_dataframe, dataframe_features\n\u001b[32m    202\u001b[39m     )\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_group._id:\n\u001b[32m    205\u001b[39m     \u001b[38;5;66;03m# only save metadata if feature group does not exist\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_feature_group_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# else, just verify that feature group schema matches user-provided dataframe\u001b[39;00m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m._verify_schema_compatibility(\n\u001b[32m    212\u001b[39m         feature_group.features, dataframe_features\n\u001b[32m    213\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/feature_group_engine.py:539\u001b[39m, in \u001b[36mFeatureGroupEngine.save_feature_group_metadata\u001b[39m\u001b[34m(self, feature_group, dataframe_features, write_options)\u001b[39m\n\u001b[32m    526\u001b[39m     _write_options = (\n\u001b[32m    527\u001b[39m         [\n\u001b[32m    528\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: k, \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: v}\n\u001b[32m   (...)\u001b[39m\u001b[32m    533\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    534\u001b[39m     )\n\u001b[32m    535\u001b[39m     feature_group._deltastreamer_jobconf = DeltaStreamerJobConf(\n\u001b[32m    536\u001b[39m         _write_options, _spark_options\n\u001b[32m    537\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_schema_available:\n\u001b[32m    542\u001b[39m     \u001b[38;5;66;03m# create empty table to write feature schema to table path\u001b[39;00m\n\u001b[32m    543\u001b[39m     \u001b[38;5;28mself\u001b[39m.save_empty_table(feature_group, write_options=write_options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/feature_group_api.py:65\u001b[39m, in \u001b[36mFeatureGroupApi.save\u001b[39m\u001b[34m(self, feature_group_instance)\u001b[39m\n\u001b[32m     60\u001b[39m query_params = {\n\u001b[32m     61\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexpand\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexpectationsuite\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformationfunctions\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     62\u001b[39m }\n\u001b[32m     63\u001b[39m headers = {\u001b[33m\"\u001b[39m\u001b[33mcontent-type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     64\u001b[39m feature_group_object = feature_group_instance.update_from_response_json(\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feature_group_object\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hopsworks_common/decorators.py:48\u001b[39m, in \u001b[36mconnected.<locals>.if_connected\u001b[39m\u001b[34m(inst, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst._connected:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hopsworks_common/client/base.py:186\u001b[39m, in \u001b[36mClient._send_request\u001b[39m\u001b[34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001b[39m\n\u001b[32m    181\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._retry_token_expired(\n\u001b[32m    182\u001b[39m         request, stream, \u001b[38;5;28mself\u001b[39m.TOKEN_EXPIRED_RETRY_INTERVAL, \u001b[32m1\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code // \u001b[32m100\u001b[39m != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.RestAPIError(url, response)\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mRestAPIError\u001b[39m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/1267872/featurestores/1254483/featuregroups). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"errorCode\":270066,\"usrMsg\":\"project: ScalableMLDL, database: scalablemldl, db user:ScalableMLDL_emamino0, jdbcString: jdbc:mysql://10.2.8.180:3306/scalablemldl?useSSL=false&allowPublicKeyRetrieval=true\",\"devMsg\":\"Access denied for user \\'ScalableMLDL_emamino0\\'@\\'10-2-5-109.hopsworks-internal.hopsworks.svc.cluster.local\\' (using password: YES)\",\"errorMsg\":\"Could not initiate connection to MySQL Server\"}', error code: 270066, error msg: Could not initiate connection to MySQL Server, user msg: project: ScalableMLDL, database: scalablemldl, db user:ScalableMLDL_emamino0, jdbcString: jdbc:mysql://10.2.8.180:3306/scalablemldl?useSSL=false&allowPublicKeyRetrieval=true"
     ]
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare Data: Ensure we have both String ID and Timestamp Event Time\n",
    "df_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\n",
    "\n",
    "# Get the Feature Store handle\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# Create Feature Group\n",
    "weather_fg = fs.get_or_create_feature_group(\n",
    "    name=\"weather_measurements\",\n",
    "    version=1,\n",
    "    primary_key=['country', 'city'],       # String ID for Online Store\n",
    "    event_time=\"date\",              # Timestamp for Point-in-Time Correctness\n",
    "    description=\"Daily weather data for Stockholm\",\n",
    "    online_enabled=True\n",
    ")\n",
    "\n",
    "# Create Expectation Suite\n",
    "weather_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "# Add Expectations\n",
    "# --- Temperature (roughly -40 to +40 for Stockholm extremes) ---\n",
    "for temp_col in [\"temperature_2m_max\", \"temperature_2m_min\", \"temperature_2m_mean\"]:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_between\",\n",
    "            kwargs={\"column\": temp_col, \"min_value\": -50.0, \"max_value\": 50.0}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- Physical quantities (Rain, Snow, Wind) cannot be negative ---\n",
    "non_negative_cols = [\n",
    "    \"precipitation_sum\", \"rain_sum\", \"snowfall_sum\", \"wind_speed_10m_max\"\n",
    "]\n",
    "for col in non_negative_cols:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_between\",\n",
    "            kwargs={\"column\": col, \"min_value\": 0.0, \"max_value\": 1000.0} # Cap at 1000 to catch errors\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- Wind Direction (0-360 degrees) ---\n",
    "weather_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"wind_direction_10m_dominant\", \"min_value\": 0.0, \"max_value\": 360.0}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save Expectations\n",
    "weather_fg.save_expectation_suite(expectation_suite=weather_expectation_suite)\n",
    "print(\"Expectation Suite created and attached.\")\n",
    "\n",
    "# Insert Data\n",
    "weather_fg.insert(df_weather)\n",
    "\n",
    "# Update Feature Descriptions\n",
    "weather_fg.update_feature_description(\"date\", \"Date of the weather measurement (Timestamp)\")\n",
    "weather_fg.update_feature_description(\"date_str\", \"Date string (YYYY-MM-DD) used as Primary Key\")\n",
    "weather_fg.update_feature_description(\"temperature_2m_max\", \"Maximum daily air temperature at 2 meters (¬∞C)\")\n",
    "weather_fg.update_feature_description(\"temperature_2m_min\", \"Minimum daily air temperature at 2 meters (¬∞C)\")\n",
    "weather_fg.update_feature_description(\"temperature_2m_mean\", \"Mean daily air temperature at 2 meters (¬∞C)\")\n",
    "weather_fg.update_feature_description(\"precipitation_sum\", \"Sum of daily precipitation (rain + showers + snow) (mm)\")\n",
    "weather_fg.update_feature_description(\"rain_sum\", \"Sum of daily rain (mm)\")\n",
    "weather_fg.update_feature_description(\"snowfall_sum\", \"Sum of daily snowfall (cm)\")\n",
    "weather_fg.update_feature_description(\"wind_speed_10m_max\", \"Maximum wind speed gusts at 10 meters (km/h)\")\n",
    "weather_fg.update_feature_description(\"wind_direction_10m_dominant\", \"Dominant wind direction at 10 meters (¬∞)\")\n",
    "weather_fg.update_feature_description(\"weather_code\", \"WMO Weather code (0-99) indicating general conditions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
