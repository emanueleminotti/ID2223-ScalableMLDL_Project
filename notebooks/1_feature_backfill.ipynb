{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1f2881-7273-45ff-9259-537b23ad8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Root dir: /Users/emaminotti/ID2223-ScalableMLDL_Project\n",
      "Added the following directory to the PYTHONPATH: /Users/emaminotti/ID2223-ScalableMLDL_Project\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "root_dir = Path().absolute()\n",
    "# Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "if root_dir.parts[-1:] == ('notebooks',):\n",
    "    root_dir = Path(*root_dir.parts[:-1])\n",
    "root_dir = str(root_dir) \n",
    "print(\"Local environment\")\n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` \n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "\n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6a80c",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f447120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "import util\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b8d1a-62a5-4a1d-b805-6e83cafcd29f",
   "metadata": {},
   "source": [
    "## Hopsworks API Key\n",
    "You need to have registered an account on app.hopsworks.ai.\n",
    "\n",
    "Save the HOPSWORKS_API_KEY  to ~/.env file in the root directory of your project\n",
    "\n",
    "In the .env file, update HOPSWORKS_API_KEY:\n",
    "\n",
    "`HOPSWORKS_API_KEY=\"put API KEY value in this string\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1a49d6-9cd2-4246-b0ca-1058672e4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-20 15:43:10,313 INFO: Initializing external client\n",
      "2025-12-20 15:43:10,314 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-20 15:43:11,767 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1267872\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9145f0b7-d961-41f7-aebe-741dbf00784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hopsworks import RestAPIError\n",
    "\n",
    "api_url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "country = \"Sweden\"\n",
    "city = \"Stockholm\"\n",
    "latitude = 59.3346 \n",
    "longitude = 18.0632"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706e751",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Read the historical data into a DataFrame </span>\n",
    "\n",
    "The cell below will read up historical pollen levels data into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3a1212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pollen data for Stockholm from 2013-01-01 to 2025-12-20...\n",
      "Data fetched successfully!\n",
      "                 time  alder_pollen  birch_pollen  grass_pollen  \\\n",
      "0 2013-01-01 00:00:00           NaN           NaN           NaN   \n",
      "1 2013-01-01 01:00:00           NaN           NaN           NaN   \n",
      "2 2013-01-01 02:00:00           NaN           NaN           NaN   \n",
      "3 2013-01-01 03:00:00           NaN           NaN           NaN   \n",
      "4 2013-01-01 04:00:00           NaN           NaN           NaN   \n",
      "\n",
      "   mugwort_pollen  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "Total rows: 113688\n"
     ]
    }
   ],
   "source": [
    "# Define the date range for historical data\n",
    "start_date = \"2013-01-01\"\n",
    "end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the pollen variables available in Open-Meteo\n",
    "# We don't consider olive and ragweed because they are negligible\n",
    "pollen_vars = [\n",
    "    \"alder_pollen\", \n",
    "    \"birch_pollen\", \n",
    "    \"grass_pollen\", \n",
    "    \"mugwort_pollen\"\n",
    "]\n",
    "\n",
    "# Set up the API parameters\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"hourly\": \",\".join(pollen_vars), # Request all pollen types\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"timezone\": \"auto\" # Automatically match the city's timezone\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "print(f\"Fetching pollen data for {city} from {start_date} to {end_date}...\")\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    response_json = response.json()\n",
    "    \n",
    "    # Convert the 'hourly' data into a Pandas DataFrame\n",
    "    hourly_data = response_json[\"hourly\"]\n",
    "    df_pollen = pd.DataFrame(hourly_data)\n",
    "    \n",
    "    # Convert the 'time' column to datetime objects and set it as the index\n",
    "    df_pollen[\"time\"] = pd.to_datetime(df_pollen[\"time\"])\n",
    "    \n",
    "    print(\"Data fetched successfully!\")\n",
    "    print(df_pollen.head())\n",
    "    print(f\"Total rows: {len(df_pollen)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812eb37-04e3-4291-8d77-a69ef7a195bc",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Data cleaning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd20c859-ef3c-4b54-bbcb-83898afefa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and interpolation complete.\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns\n",
    "cols_to_keep = ['time', 'alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']\n",
    "df_pollen_clean = df_pollen[cols_to_keep].copy()\n",
    "\n",
    "# Set index and Resample to Daily Mean\n",
    "df_pollen_clean.set_index('time', inplace=True)\n",
    "df_pollen_daily = df_pollen_clean.resample('D').mean()\n",
    "\n",
    "# Linear Interpolation for gaps\n",
    "# limit_direction='both' ensures we fill gaps based on surrounding data\n",
    "df_pollen_daily = df_pollen_daily.interpolate(method='time', limit_direction='both')\n",
    "\n",
    "df_pollen_daily.dropna(inplace=True)\n",
    "\n",
    "# Cast types\n",
    "pollen_cols = ['alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']\n",
    "df_pollen_daily[pollen_cols] = df_pollen_daily[pollen_cols].astype('float32')\n",
    "\n",
    "# Reset index\n",
    "df_pollen_daily.reset_index(inplace=True)\n",
    "df_pollen_daily.rename(columns={'time': 'date'}, inplace=True)\n",
    "\n",
    "print(\"Data cleaning and interpolation complete.\")\n",
    "print(f\"Remaining missing values: {df_pollen_daily.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa32b9",
   "metadata": {},
   "source": [
    "### Adding lagged features\n",
    "\n",
    "To capture short-term temporal dependencies, we add three new features representing the pollen levels of the previous 1, 2, and 3 days.\n",
    "These lagged values will help the model learn patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba7dfa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with lagged features:\n",
      "        date  birch_pollen  birch_pollen_lag_1  birch_pollen_lag_2\n",
      "0 2013-01-04           0.0                 0.0                 0.0\n",
      "1 2013-01-05           0.0                 0.0                 0.0\n",
      "2 2013-01-06           0.0                 0.0                 0.0\n",
      "3 2013-01-07           0.0                 0.0                 0.0\n",
      "4 2013-01-08           0.0                 0.0                 0.0\n",
      "\n",
      "New DataFrame shape: (4734, 17)\n"
     ]
    }
   ],
   "source": [
    "# Define the pollen columns we want to lag\n",
    "pollen_cols = ['alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']\n",
    "\n",
    "# Create the lagged features\n",
    "for col in pollen_cols:\n",
    "    for lag in range(1, 4):  # Loops through 1, 2, 3\n",
    "        # Name the new column, e.g., 'alder_pollen_lag_1'\n",
    "        lag_col_name = f\"{col}_lag_{lag}\"\n",
    "        # Shift the column data down by 'lag' rows\n",
    "        df_pollen_daily[lag_col_name] = df_pollen_daily[col].shift(lag)\n",
    "\n",
    "# Drop the first 3 rows which now contain NaNs (since we shifted up to 3 days)\n",
    "df_pollen_daily.dropna(inplace=True)\n",
    "\n",
    "# Reset index is generally good practice after dropping rows, though not strictly necessary if date is a column\n",
    "df_pollen_daily.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check the result\n",
    "print(\"Data with lagged features:\")\n",
    "print(df_pollen_daily[['date', 'birch_pollen', 'birch_pollen_lag_1', 'birch_pollen_lag_2']].head())\n",
    "print(f\"\\nNew DataFrame shape: {df_pollen_daily.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e8276",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055befa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style='color:#ff5f27'> Loading Weather Data from [Open Meteo](https://open-meteo.com/en/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78686a28",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Download the Historical Weather Data </span>\n",
    "\n",
    "\n",
    "We will download the historical weather data for your `city` by first extracting the earliest date from your DataFrame containing the historical air quality measurements.\n",
    "\n",
    "We will download all daily historical weather data measurements for your `city` from the earliest date in your air quality measurement DataFrame. It doesn't matter if there are missing days of air quality measurements. We can store all of the daily weather measurements, and when we build our training dataset, we will join up the air quality measurements for a given day to its weather features for that day. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d604b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching historical weather data from 2013-01-04 to 2025-12-20...\n",
      "Weather data fetched successfully!\n",
      "        date  temperature_2m_max  temperature_2m_min  temperature_2m_mean  \\\n",
      "0 2013-01-04                 2.7                -0.3                  1.6   \n",
      "1 2013-01-05                -0.4                -3.6                 -2.2   \n",
      "2 2013-01-06                -0.2                -3.8                 -1.7   \n",
      "3 2013-01-07                 0.8                -4.8                 -1.2   \n",
      "4 2013-01-08                 1.3                -2.5                  0.0   \n",
      "\n",
      "   precipitation_sum  rain_sum  snowfall_sum  wind_speed_10m_max  \\\n",
      "0                0.0       0.0          0.00           23.200001   \n",
      "1                0.0       0.0          0.00           16.900000   \n",
      "2                0.0       0.0          0.00            9.400000   \n",
      "3                0.0       0.0          0.00           15.100000   \n",
      "4                2.5       0.1          1.75           18.400000   \n",
      "\n",
      "   wind_direction_10m_dominant  weather_code  \n",
      "0                        309.0           3.0  \n",
      "1                        322.0           3.0  \n",
      "2                         30.0           3.0  \n",
      "3                        219.0           3.0  \n",
      "4                        247.0          73.0  \n",
      "Shape: (4734, 10)\n"
     ]
    }
   ],
   "source": [
    "# Get the earliest date from the pollen DataFrame\n",
    "start_date = df_pollen_daily['date'].min().strftime('%Y-%m-%d')\n",
    "end_date = datetime.date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Fetching historical weather data from {start_date} to {end_date}...\")\n",
    "\n",
    "# Define the Weather API URL (Archive API for history)\n",
    "weather_api_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define relevant weather variables for pollen prediction\n",
    "weather_vars = [\n",
    "    \"temperature_2m_max\",\n",
    "    \"temperature_2m_min\", \n",
    "    \"temperature_2m_mean\",\n",
    "    \"precipitation_sum\",\n",
    "    \"rain_sum\",\n",
    "    \"snowfall_sum\",\n",
    "    \"wind_speed_10m_max\",\n",
    "    \"wind_direction_10m_dominant\",\n",
    "    \"weather_code\"\n",
    "]\n",
    "\n",
    "# Set up the API parameters\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"daily\": \",\".join(weather_vars),\n",
    "    \"timezone\": \"auto\"\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(weather_api_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_json = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    daily_data = response_json[\"daily\"]\n",
    "    df_weather = pd.DataFrame(daily_data)\n",
    "    \n",
    "    # Data Cleaning for Weather Data\n",
    "    # Convert 'time' to datetime\n",
    "    df_weather[\"time\"] = pd.to_datetime(df_weather[\"time\"])\n",
    "    df_weather.rename(columns={\"time\": \"date\"}, inplace=True)\n",
    "    \n",
    "    # Cast float columns to float32 to save memory\n",
    "    float_cols = [c for c in df_weather.columns if c != \"date\"]\n",
    "    df_weather[float_cols] = df_weather[float_cols].astype(\"float32\")\n",
    "    \n",
    "    print(\"Weather data fetched successfully!\")\n",
    "    print(df_weather.head())\n",
    "    print(f\"Shape: {df_weather.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to fetch weather data. Status: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6eefe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4734 entries, 0 to 4733\n",
      "Data columns (total 10 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         4734 non-null   datetime64[ns]\n",
      " 1   temperature_2m_max           4734 non-null   float32       \n",
      " 2   temperature_2m_min           4734 non-null   float32       \n",
      " 3   temperature_2m_mean          4734 non-null   float32       \n",
      " 4   precipitation_sum            4734 non-null   float32       \n",
      " 5   rain_sum                     4734 non-null   float32       \n",
      " 6   snowfall_sum                 4734 non-null   float32       \n",
      " 7   wind_speed_10m_max           4734 non-null   float32       \n",
      " 8   wind_direction_10m_dominant  4734 non-null   float32       \n",
      " 9   weather_code                 4734 non-null   float32       \n",
      "dtypes: datetime64[ns](1), float32(9)\n",
      "memory usage: 203.5 KB\n"
     ]
    }
   ],
   "source": [
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f77aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e79b3f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> Create the Feature Groups and insert the DataFrames in them </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2bb403",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Primary key column date_str is missing in input dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m pollen_fg = fs.get_or_create_feature_group(\n\u001b[32m      3\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33mpollen_measurements\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     description=\u001b[33m'\u001b[39m\u001b[33mDaily average pollen levels for Stockholm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     event_time=\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Insert the DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mpollen_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pollen_daily\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Add Descriptions for Documentation\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# It is best practice to document units (grains/m¬≥) so others understand the values.\u001b[39;00m\n\u001b[32m     15\u001b[39m pollen_fg.update_feature_description(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDate of the pollen measurement (Timestamp)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/feature_group.py:3454\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m   3445\u001b[39m     [\n\u001b[32m   3446\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3450\u001b[39m ):\n\u001b[32m   3451\u001b[39m     \u001b[38;5;66;03m# New delta FG allow for change data capture query\u001b[39;00m\n\u001b[32m   3452\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33mdelta.enableChangeDataFeed\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3454\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3455\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3457\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[38;5;66;03m# Compute stats in client if there is no backfill job:\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;66;03m# - spark engine: always compute in client\u001b[39;00m\n\u001b[32m   3468\u001b[39m \u001b[38;5;66;03m# - python engine: only compute if FG is offline only (no backfill job)\u001b[39;00m\n\u001b[32m   3469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/feature_group_engine.py:200\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    189\u001b[39m util.validate_embedding_feature_type(\n\u001b[32m    190\u001b[39m     feature_group.embedding_index, dataframe_features\n\u001b[32m    191\u001b[39m )\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation_options \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    194\u001b[39m     validation_options.get(\n\u001b[32m    195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monline_schema_validation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m ):\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m# validate df schema\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     dataframe_features = \u001b[43mDataFrameValidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_features\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feature_group._id:\n\u001b[32m    205\u001b[39m     \u001b[38;5;66;03m# only save metadata if feature group does not exist\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28mself\u001b[39m.save_feature_group_metadata(\n\u001b[32m    207\u001b[39m         feature_group, dataframe_features, write_options\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/schema_validation.py:70\u001b[39m, in \u001b[36mDataFrameValidator.validate_schema\u001b[39m\u001b[34m(self, feature_group, df, df_features)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# If this is the base class and not a subclass instance being called directly,\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# delegate to the appropriate subclass\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m == DataFrameValidator:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m errors = {}\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Check if the primary key columns exist\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ID2223-ScalableMLDL_Project/venv/lib/python3.11/site-packages/hsfs/core/schema_validation.py:75\u001b[39m, in \u001b[36mDataFrameValidator.validate_schema\u001b[39m\u001b[34m(self, feature_group, df, df_features)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pk \u001b[38;5;129;01min\u001b[39;00m feature_group.primary_key:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pk \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     76\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrimary key column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is missing in input dataframe\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m         )\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Execute data type specific validation\u001b[39;00m\n\u001b[32m     79\u001b[39m errors, column_lengths, is_pk_null, is_string_length_exceeded = (\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_df_specifics(feature_group, df)\n\u001b[32m     81\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Primary key column date_str is missing in input dataframe"
     ]
    }
   ],
   "source": [
    "# Get or Create the Feature Group\n",
    "pollen_fg = fs.get_or_create_feature_group(\n",
    "    name='pollen_measurements',\n",
    "    description='Daily average pollen levels for Stockholm',\n",
    "    version=1,\n",
    "    primary_key=['date'],\n",
    "    event_time=\"date\" \n",
    ")\n",
    "\n",
    "# Insert the DataFrame\n",
    "pollen_fg.insert(df_pollen_daily)\n",
    "\n",
    "# Add Descriptions for Documentation\n",
    "# It is best practice to document units (grains/m¬≥) so others understand the values.\n",
    "pollen_fg.update_feature_description(\"date\", \"Date of the pollen measurement (Timestamp)\")\n",
    "pollen_fg.update_feature_description(\"alder_pollen\", \"Daily average Alder pollen concentration (grains/m¬≥)\")\n",
    "pollen_fg.update_feature_description(\"birch_pollen\", \"Daily average Birch pollen concentration (grains/m¬≥)\")\n",
    "pollen_fg.update_feature_description(\"grass_pollen\", \"Daily average Grass pollen concentration (grains/m¬≥)\")\n",
    "pollen_fg.update_feature_description(\"mugwort_pollen\", \"Daily average Mugwort pollen concentration (grains/m¬≥)\")\n",
    "\n",
    "# Add descriptions for the lagged features\n",
    "for pollen_type in ['alder_pollen', 'birch_pollen', 'grass_pollen', 'mugwort_pollen']:\n",
    "    for i in range(1, 4):\n",
    "        col_name = f\"{pollen_type}_lag_{i}\"\n",
    "        if col_name in df_pollen_daily.columns:\n",
    "            pollen_fg.update_feature_description(col_name, f\"{pollen_type} concentration {i} day(s) prior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894b731",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a84ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group 'weather_measurements' recuperato o creato.\n",
      "Validazioni meteo configurate.\n",
      "Caricamento dati meteo su Hopsworks...\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1279152/fs/1258616/fg/1866063\n",
      "2025-12-15 15:57:33,417 INFO: \t11 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1279152/fs/1258616/fg/1866063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 4732/4732 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_measurements_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1279152/jobs/named/weather_measurements_1_offline_fg_materialization/executions\n",
      "Dati meteo caricati e validati con successo!\n"
     ]
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Prepare Data: Ensure we have both String ID and Timestamp Event Time\n",
    "# We assume 'df_weather' was created in the previous step\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "df_weather['date'] = df_weather['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 2. Get the Feature Store handle\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# 3. Create Feature Group\n",
    "# We use version 1 since this is a new feature group\n",
    "weather_fg = fs.get_or_create_feature_group(\n",
    "    name=\"weather_measurements\",\n",
    "    version=1,\n",
    "    primary_key=[\"date\"],       # String ID for Online Store\n",
    "    event_time=\"date\",              # Timestamp for Point-in-Time Correctness\n",
    "    description=\"Daily weather data for Stockholm (Open-Meteo)\",\n",
    "    online_enabled=True\n",
    ")\n",
    "\n",
    "# Create Expectation Suite\n",
    "weather_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "# Add Expectations\n",
    "# --- Temperature (roughly -40 to +40 for Stockholm extremes) ---\n",
    "for temp_col in [\"temperature_2m_max\", \"temperature_2m_min\", \"temperature_2m_mean\"]:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_between\",\n",
    "            kwargs={\"column\": temp_col, \"min_value\": -50.0, \"max_value\": 50.0}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- Physical quantities (Rain, Snow, Wind) cannot be negative ---\n",
    "non_negative_cols = [\n",
    "    \"precipitation_sum\", \"rain_sum\", \"snowfall_sum\", \"wind_speed_10m_max\"\n",
    "]\n",
    "for col in non_negative_cols:\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_values_to_be_between\",\n",
    "            kwargs={\"column\": col, \"min_value\": 0.0, \"max_value\": 1000.0} # Cap at 1000 to catch errors\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- Wind Direction (0-360 degrees) ---\n",
    "weather_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_between\",\n",
    "        kwargs={\"column\": \"wind_direction_10m_dominant\", \"min_value\": 0.0, \"max_value\": 360.0}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save Expectations\n",
    "weather_fg.save_expectation_suite(expectation_suite=weather_expectation_suite)\n",
    "print(\"Expectation Suite created and attached.\")\n",
    "\n",
    "# Insert Data\n",
    "weather_fg.insert(df_weather)\n",
    "\n",
    "# Update Feature Descriptions\n",
    "weather_fg.update_feature_description(\"date\", \"Date of the weather measurement (Timestamp)\")\n",
    "weather_fg.update_feature_description(\"date_str\", \"Date string (YYYY-MM-DD) used as Primary Key\")\n",
    "weather_fg.update_feature_description(\"temperature_2m_max\", \"Maximum daily air temperature at 2 meters (¬∞C)\")\n",
    "weather_fg.update_feature_description(\"temperature_2m_min\", \"Minimum daily air temperature at 2 meters (¬∞C)\")\n",
    "weather_fg.update_feature_description(\"temperature_2m_mean\", \"Mean daily air temperature at 2 meters (¬∞C)\")\n",
    "weather_fg.update_feature_description(\"precipitation_sum\", \"Sum of daily precipitation (rain + showers + snow) (mm)\")\n",
    "weather_fg.update_feature_description(\"rain_sum\", \"Sum of daily rain (mm)\")\n",
    "weather_fg.update_feature_description(\"snowfall_sum\", \"Sum of daily snowfall (cm)\")\n",
    "weather_fg.update_feature_description(\"wind_speed_10m_max\", \"Maximum wind speed gusts at 10 meters (km/h)\")\n",
    "weather_fg.update_feature_description(\"wind_direction_10m_dominant\", \"Dominant wind direction at 10 meters (¬∞)\")\n",
    "weather_fg.update_feature_description(\"weather_code\", \"WMO Weather code (0-99) indicating general conditions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
